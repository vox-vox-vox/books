# 仿真调试配置

# 1. hx 命名规则

普通流程：hx_simpleTask

​					hx_simpleFailTask



动态子流程测试：一切以hx_dynamic开头

父流程：hx_dynamic_fatherWorkflow

​	创建output的：hx_dynamic_makerTask

​	创建子流程的：create_dynamic_subworkflow，这个是公用的，会被pod拉走

​							   hx_dynamic_subworkflowCreaterTask，这个是一样的，不会被pod拉走

子流程：hx_dynamic_subWorkflow





Query:(以hhxx结尾)

A--B--C: hx_testQuery_hhxx





# 2. 命名规则

ref是流程属性，为了区分一个流程中相同的task

普通name是查询属性，为了查出来这个东西而已

无论是对于task还是对于workflow，上述规则都适用





# 3. 日志输出

更改日志输出等级，有利于调试

mlog.go 文件里面

```go
var Glogger = GetLogger("mojito", zap.ErrorLevel)
```



# 4. 直接测试接口

http://localhost:7900/api/v1/workflow-defs/delete



# 5. 环境

```go
env: dev
mpushURL: "http://dev-mpush.hdmap.momenta.works"
taskCheckTTL: 5m
taskCycleCheckAlarm:
  queueTimeoutDuration: 4h
  executeTimeoutDuration: 2h
  checkTimeSeconds: 7200

cassandra:
  hosts: cds-2zeu1lplf9q86rq2-1-core-001.cassandra.rds.aliyuncs.com,cds-2zeu1lplf9q86rq2-1-core-002.cassandra.rds.aliyuncs.com,cds-2zeu1lplf9q86rq2-1-core-003.cassandra.rds.aliyuncs.com
  port: 9042
  keyspace: cadence_dev
  user: cassandra
  password: Momenta123


redisConfig:
  addr: "redis:6379"
  addrs: 
    - "op1.hdmap.mmtdev.com:26380"
    - "op2.hdmap.mmtdev.com:26380"
    - "prod-pg.hdmap.mmtdev.com:26380"
  password: "hdmap@redis!23"
  masterName: "mymaster"


gracefulShutdown:
  timeoutSeconds: 5
postgresConfig:
  host: "hdmap-nonprod.pg.rds.aliyuncs.com"
  #  host: "hdmap-prod.pg.rds.aliyuncs.com"
  #  host: "prod-pg.hdmap.mmtdev.com"
  dbname: "muniu_dev"
  port: 5432
  user: "postgres"
  passwd: "HDMAP@2019momenta"
  maxOpenConns: 2000
testPostgresConfig:
  host: "hdmap-nonprod.pg.rds.aliyuncs.com"
  #  host: "hdmap-prod.pg.rds.aliyuncs.com"
  #  host: "prod-pg.hdmap.mmtdev.com"
  dbname: "muniu"
  port: 5432
  user: "postgres"
  passwd: "HDMAP@2019momenta"
  maxOpenConns: 2000
cadence:
  #  frontendService: op1.hdmap.mmtdev.com:7933
  #frontendService: dev-cadence.hdmap.mmtdev.com:30013
  frontendService: 10.247.168.220:30013
  #  frontendService: staging-cadence.hdmap.mmtdev.com:30023
  #  frontendService: cadence.hdmap.mmtdev.com:30033
  #  frontendService: 172.21.65.94:7933
  domain: hdmap-workflow
  workflowStartToCloseTimeout: 1440h #time.Hour * 24 * 60
  decisionTaskStartToCloseTimeout: 1m
  uiURL: http://op1.hdmap.mmtdev.com:8088/
conductorConfig:
  serverURL: http://dev-conductor-server.hdmap.momenta.works/
productionMonitorURL: http://dev-production-monitor.hdmap.momenta.works
elasticSearchURL: http://elastic:hdmap!23@es-cn-v0h1ktqfx000i4t8v.elasticsearch.aliyuncs.com:9200/
kibanaURL: https://es-cn-v0h1ktqfx000i4t8v.kibana.elasticsearch.aliyuncs.com:5601/app/kibana#/discover?_g=(filters:!(),refreshInterval:(pause:!t,value:0),
kibanaIndexMap:
  dev: "9a313b40-0ed6-11eb-8101-e3a185b4a18d"
  staging: "b0bd7a90-0ed6-11eb-8101-e3a185b4a18d"
  production: "779b03a0-6cf0-11ea-a2ad-cd4da7e8b0de"
  prod: "779b03a0-6cf0-11ea-a2ad-cd4da7e8b0de"
  local: "9a313b40-0ed6-11eb-8101-e3a185b4a18d"
logESIndexMap:
  dev: "devk8s-hdmap-dev-"
  staging: "devk8s-hdmap-staging-"
  production: "hdmap-prodiction-"
  prod: "hdmap-prodiction-"
  local: "devk8s-hdmap-dev-"
mojitoURL: http://dev-mojito.hdmap.momenta.works
```

前端：

```shell
# pm-ui
make local
# production management ui
make local
```



# 6. 真正的配置看这里

![image-20210826204703560](/Users/huxiao/Library/Application Support/typora-user-images/image-20210826204703560.png)



# 7. 新开的cadence的docker-file

```go
version: '3'
services:
  cassandra:
    image: cassandra:3.11
    ports:
      - "5042:9042"
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus_config.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    ports:
      - '5090:9090'
  node-exporter:
    image: prom/node-exporter
    ports:
      - '5100:9100'
  cadence:
    image: ubercadence/server:master-auto-setup
    ports:
     - "5000:8000"
     - "5001:8001"
     - "5002:8002"
     - "5003:8003"
     - "5933:7933"
     - "5934:7934"
     - "5935:7935"
     - "5939:7939"
     - "5833:7833"
    environment:
      - "CASSANDRA_SEEDS=cassandra"
      - "PROMETHEUS_ENDPOINT_0=0.0.0.0:8000"
      - "PROMETHEUS_ENDPOINT_1=0.0.0.0:8001"
      - "PROMETHEUS_ENDPOINT_2=0.0.0.0:8002"
      - "PROMETHEUS_ENDPOINT_3=0.0.0.0:8003"
      - "DYNAMIC_CONFIG_FILE_PATH=config/dynamicconfig/development.yaml"
    depends_on:
      - cassandra
      - prometheus
  cadence-web:
    image: ubercadence/web:latest
    environment:
      - "CADENCE_TCHANNEL_PEERS=cadence:7933"
    ports:
      - "5088:8088"
    depends_on:
      - cadence
  grafana:
    image: grafana/grafana
    user: "1000"
    depends_on:
      - prometheus
    ports:
      - '3000:3000'
```



利用docker远程执行domain的注册

```shell
docker run -it --rm ubercadence/cli:master --address 172.21.65.94:5933 --do samples-domain domain register
```

# 8. Gopprof 查看goroutine

```go
1. 开启http监听服务

package main

import (
        _ "net/http/pprof"
        "net/http"

)

func main() {

        go func() {
                log.Println(http.ListenAndServe("localhost:6060", nil))
        }()

        select{}
}

2. 进行采样与分析

go tool pprof http://localhost:6060/debug/pprof/goroutine/profile\?seconds\=20

3. 查看结果
go tool pprof -http=:8082 ~/pprof/pprof.samples.cpu.005.pb.gz
```



# 9. 纯净cadence开发环境yaml

systemWorker：

```yaml
env: local
productionManagementURL: http://localhost:8080/
mojitoServerURL: http://localhost:7900/
#productionManagementURL: http://dev-production-management.hdmap.momenta.works/
cadence:
  frontendService: 172.21.65.94:7933
  domain: hdmap-workflow
  workflowStartToCloseTimeout: 1440h #time.Hour * 24 * 60
  decisionTaskStartToCloseTimeout: 1m
  uiURL: http://172.21.65.94:8088/

jobScheduler:
  -
    name: "xiangru"
    image: "artifactory.momenta.works/docker-hdmap-dev/test-pyworker:dev-20210128.7"
    cmd: "python3 worker.py"
  -
    name: test
    image: test
    cmd: test

```

mojito：

```yaml
env: dev
mpushURL: "http://dev-mpush.hdmap.momenta.works"
engines:
  - cadence
taskCheckTTL: 5m
taskCycleCheckAlarm:
  queueTimeoutDuration: 4h
  executeTimeoutDuration: 2h
  checkTimeSeconds: 7200

cassandra:
  hosts: cds-2zeu1lplf9q86rq2-1-core-001.cassandra.rds.aliyuncs.com,cds-2zeu1lplf9q86rq2-1-core-002.cassandra.rds.aliyuncs.com,cds-2zeu1lplf9q86rq2-1-core-003.cassandra.rds.aliyuncs.com
  port: 9042
  keyspace: cadence_dev
  user: cassandra
  password: Momenta123


redisConfig:
  addr: "redis:6379"
  addrs: 
    - "op1.hdmap.mmtdev.com:26380"
    - "op2.hdmap.mmtdev.com:26380"
    - "prod-pg.hdmap.mmtdev.com:26380"
  password: "hdmap@redis!23"
  masterName: "mymaster"


gracefulShutdown:
  timeoutSeconds: 5
postgresConfig:
  host: "hdmap-nonprod.pg.rds.aliyuncs.com"
  dbname: "muniu_dev"
  port: 5432
  user: "postgres"
  passwd: "HDMAP@2019momenta"
  maxOpenConns: 2000
testPostgresConfig:
  host: "hdmap-nonprod.pg.rds.aliyuncs.com"
  dbname: "muniu"
  port: 5432
  user: "postgres"
  passwd: "HDMAP@2019momenta"
  maxOpenConns: 2000
cadence:
  frontendService: 172.21.65.94:7933
  domain: hdmap-workflow
  workflowStartToCloseTimeout: 1440h #time.Hour * 24 * 60
  decisionTaskStartToCloseTimeout: 1m
  uiURL: http://172.21.65.94:8088/
conductorConfig:
  serverURL: http://dev-conductor-server.hdmap.momenta.works/
productionMonitorURL: http://dev-production-monitor.hdmap.momenta.works
productionManagementURL: http://dev-production-management.hdmap.momenta.works
elasticSearchURL: http://elastic:hdmap!23@es-cn-v0h1ktqfx000i4t8v.elasticsearch.aliyuncs.com:9200/
kibanaURL: https://es-cn-v0h1ktqfx000i4t8v.kibana.elasticsearch.aliyuncs.com:5601/app/kibana#/discover?_g=(filters:!(),refreshInterval:(pause:!t,value:0),
kibanaIndexMap:
  dev: "9a313b40-0ed6-11eb-8101-e3a185b4a18d"
  staging: "b0bd7a90-0ed6-11eb-8101-e3a185b4a18d"
  production: "779b03a0-6cf0-11ea-a2ad-cd4da7e8b0de"
  prod: "779b03a0-6cf0-11ea-a2ad-cd4da7e8b0de"
  local: "9a313b40-0ed6-11eb-8101-e3a185b4a18d"
logESIndexMap:
  dev: "devk8s-hdmap-dev-"
  staging: "devk8s-hdmap-staging-"
  production: "hdmap-prodiction-"
  prod: "hdmap-prodiction-"
  local: "devk8s-hdmap-dev-"
mojitoURL: http://dev-mojito.hdmap.momenta.works
k8sConfig:
  tolerates:
    - Key:      "large"
      Effect:   "NoSchedule"
      Value:    "true"
      Operator: "Equal"
    - Key:      "dev"
      Effect:   "NoSchedule"
      Value:    "true"
      Operator: "Equal"
  mem: 200
  tasksParam:

    IwMcvLidar_IwMcvLidarHigh:
      convertLengthRatio: 1000
      cpu: 10
      # minCPU: 未定义
      # maxCPU: 未定义
      # mem:(30 + (float64(convertedLength-3)/7)*15) * 1024
      minMem: 30720
      maxMem: 46080

    IwMcvMesh_IwMcvMeshHigh:
      # cpu: 14 + (float64(convertedFileNamesNum-600)/1400)*16
      minCPU: 14
      maxCPU: 30
      # mem: (15 + (float64(convertedFileNamesNum-600)/1400)*25) * 1024
      minMem: 3072
      maxMem: 40960

    IwMcvSemantic_IwMcvSemanticHigh:
      cpu: 8
      # minCPU: 未定义
      # maxCPU: 未定义
      mem: 18000
      # minMem: 未定义
      # maxMem: 未定义

    GenerateOccupancyMap_GenerateOccupancyMapHigh:
      cpu: 10
      # minCPU: 未定义
      # maxCPU: 未定义
      # mem: (5 + (float64(convertedFileNamesNum-1000)/9000)*35) * 1024
      minMem: 5120
      maxMem: 40960

    IwMeshBoundary_IwMeshBoundaryHigh:
      # cpu: 2 + (float64(convertedSize-1024)/2048)*3
      minCPU: 2
      maxCPU: 6
      # mem: (20 + (float64(convertedSize-1024)/7168)*70) * 1024
      minMem: 20480
      maxMem: 92160


    Default:
      cpu_requests: 2
      cpu_limits: 4
      mem_requests: ${k8sConfig.mem}
      mem_limits: 500

callbackOpen: "false"
```

Pm:

```yaml
env: dev
conductorConfig:
  controlTasks: [copy, decompress, delete]
  domainList: [m14, m15, m17, m32, m33, a66, a69, a70]
  deviceList: [M1903-SKU4-V03-1911-0001, M1903-SKU4-V03-1911-0002, M1903-SKU4-V03-1911-0014, M1903-SKU4-V03-1911-0024,M1903-SKU4-V03-1911-0027, M1903-SKU4-V03-1911-0032, M1903-SKU4-V03-1912-0034,M1903-SKU4-V03-1912-0042,M1903-SKU4-V03-1912-0045, M1903-SKU4-V03-2001-0051]
  serverURL: http://dev-conductor-server.hdmap.momenta.works/api/
  uiURL: http://dev-conductor-ui.hdmap.momenta.works/
  rpcAddr: http://dev-conductor-server.hdmap.momenta.works

resourceCost:
  CPUPRICE: 0.15
  GPUPRICE: 11
  MEMORYPRICE: 0.04
  taskLastStage: iw_auto_dnn

cadence:
  frontendService: 10.247.168.220:30013
  domain: hdmap-workflow
  uiURL: http://dev-pm-ui.hdmap.momenta.works/
  workflowWorkers:
    - packageFlowSplit
    - test_workflow
    - cover_match_process
    - productionline_workflow_after_align_test_origin
  httpActivityWorkers:
    - update_status
    - update_params
    - notify_error



# default is conductor
workflow:
  map_produce_partition: cadence
  segment_produce: cadence
  crowdsourcing_segment_produce: cadence
  segment_produce_mcv2: cadence
  online_map_produce: cadence
  mcv_produce: cadence
  crowdsourcing_map_produce: cadence
  segment_produce_mcv: cadence
  map_produce: cadence
  mvc_produce_only_split: cadence

workorder:
  defaultMasterOSM: http://dev-osm-muku.hdmap.momenta.works

postgresConfig:
  dbname: muniu_dev
  host:  hdmap-nonprod.pg.rds.aliyuncs.com
  passwd: HDMAP@2019momenta
  port: 5432
  user: postgres
  maxOpenConns: 100

cassandra:
  hosts: cds-2zeu1lplf9q86rq2-1-core-001.cassandra.rds.aliyuncs.com,cds-2zeu1lplf9q86rq2-1-core-002.cassandra.rds.aliyuncs.com,cds-2zeu1lplf9q86rq2-1-core-003.cassandra.rds.aliyuncs.com
  port: 9042
  keyspace: cadence_dev
  user: cassandra
  password: Momenta123

mauth:
  clientID: production-management
  clientSecret: test123
  endpoint: http://dev-hydra.hdmap.momenta.works/
  permissionEndpoint: http://keto:4466/
  adminEndpoint: http://dev-hydra-admin.hdmap.momenta.works/
  accountEndpoint: http://dev-account-server.hdmap.momenta.works/api/v1/users/login
  callbackEndpoint: http://dev-production-management.hdmap.momenta.works/mauth/callback
  user: production-management
  pass: 79ab808c

taskCountRecord:
  enabled: true

aws:
  AccessKeyID: PGTW30YDX07W1W8FYVO7
  SecretAccessKey: jVuzFaK3xDgEp3sJN47YD0T3KXg2znk7adOrOBQb
  Endpoint: http://bjceph01.mapin.hdmap.momenta.works:80
  Region: cn-north-1
  OnlinePackageDataBucket: momenta-hdmap-online-dev
  OnlinePackageDataPath: /map-data/

s3bucket:
  owncar: raw-data-crowdsourcing
  crowdsourcing: raw-data-crowdsourcing
  mvp: raw-data-crowdsourcing

taskManagementURL: http://field-task-manage:8040
skeletonServerURL: http://skeleton-service:5000
dataWarehouseURL: http://data-warehouse:8080/api/v1/
intelligenceServerURL: http://10.8.104.221:7000
productionMonitorURL: http://production-monitor:8080/
skeletonOSMURL: http://dev-skeleton.hdmap.momenta.works/
pmServerURL: http://dev-production-management.hdmap.momenta.works/api/v1/
mppURL: http://mpp:8083
accountServerURL: http://account-server:8085
mpushURL: "http://dev-mpush.hdmap.momenta.works"
gridStoreURL: http://grid-store:6106
mojitoServerURL: http://localhost:7900
testPostgresConfig:
  dbname: muniu_test
  host: 172.21.2.86
  passwd: HDMAP@2019momenta
  port: 5433
  user: postgres
gracefulShutdown:
  timeoutSeconds: 5
workflowControlConfig:
  successRate: 0.9
  onlineSegmentTaskTriggerThreshold: 2
  onlineSegmentTaskUserSkeletons:
    - "w2953"
    - "w2990"
    - "w6594"
    - "w6595"
    - "w126"
    - "w366023"
    - "w6524"
    - "w12929"
osmServicePool:
  - portRange:
      start: 2100
      end: 2199
redisConfig:
  addrs:
    - redis:6379
productionManagementURL: http://dev-production-management.hdmap.momenta.works/
taskCycleCheckAlarm:
  checkTimeSeconds: 7200
  queueTimeoutDuration: 4h
  executeTimeoutDuration: 2h
elasticSearchURL: http://elasticsearch:9200/conductor/task/
taskCheckTTL: 1m
accountServer:
  user: "production-management@proj"
  password: "ismI83IV9tQo"
eass:
  whiteNode:
    - "10.250.76.7"
    - "10.250.76.62"
    - "10.250.76.38"
    - "10.250.76.26"
    - '10.250.76.107'
    - "10.250.76.102"
    - "10.250.76.58"
    - "10.250.76.44"
    - "10.250.76.50"
  maxPodScheduleTime: "2m"
  maxHealthTime: "360"
  maxEvictTime: "20m"
  taskLengthThreshold: 50
  limitPodNumber: 25

osm-diff-get-loc: "http://dev-grid-store.hdmap.momenta.works/checklist/with_category"
warning:
  eass: "https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=b9831259-04a4-46ae-be59-72b128d4cd22"
  human-monitor: "https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=f5cab5e4-b7ca-479d-a13f-785a8c6854c5"
```



# 10 hzy 机器连接

ssh hzy@172.21.65.94
password： hzyahui1
