# 🐭人周报

上个月的自评：

自我评价：

【业绩】

1. 完成流程定义，任务定义，段任务长度等接口编写和测试
2. 完成回调钩子的编写与可靠性测试
3. 流程图匹配进行代码重构
4. 支持一些日常其他工作，如增加权限等

【文化】

1. 主动学习cadence及cadence-system worker的工作原理
2. 由于自己的基础相对薄弱，遇到问题会整理记录下来（如网络请求方面，linux方面等）
3. 持续的危机感与创造价值的渴望，驱动潜意识不断思考事情可能出现的结果。流程图匹配代码中不断思考可能出现的各种情况



# 8.30

准备完成：

1. 在小服务器上压测并出报告，出对比图(ok)
2. 完善getflowChart2代码，尤其是skipTasks
3. 测试getFlowchart正确性（找几个用例）（ok）
4. 重新写不依赖于流程图匹配的钩子callback（ok）

今日收获：

1. 优化有效，居然最大时间消耗在网络请求上，目前看来无解。火焰图代表的是CPU消耗率，并不是CPU消耗时间，看上去数据库调用和网络请求CPU占用率差不多，实际上时间差好多，数据库大概0.0xx秒，网络请求实在是太慢了2s--10s不等



# 8.31

准备完成：

1. terminate状态显示不对，搜索结果出错。原因是每次terminate的时候并没有把SubStatus更新到cadence中，而是查询之后才进行读取。现在准备利用searchAttributes直接把他放进去（√）

   方案可行，但是存在时序问题，具体如下：

   如果更新，就必须向cadence发信号，systemWorker捕获信号，以后再向Cadence发送upsertSearchAttributes的请求，而这整个流程的时间消耗有可能大于terminate本身的时间，可能会存在先terminate才更新SearchAttributes的情况。而流程terminate后无法更新SearchAttributes，所以暂时无法解决。

2. yaml配置k8sJob

```go
Description
1. 先将配置写到helm-chart中的dev/staging/prod的ack_yaml
2. job_scheduler.go 改读取yaml
3. 配置包括：
TTLSecondsAfterFinished
Tolerations
NodeSelector
ImagePullSecrets
ResourceMemory
ResourceCPU

——————————————————————————————————
golang里：
targetJob.Spec.TTLSecondsAfterFinished
targetJob.Spec.Template.Spec.Tolerations targetJob.Spec.Template.Spec.NodeSelector
targetJob.Spec.Template.Spec.ImagePullSecrets
default：
corev1.ResourceMemory
corev1.ResourceCPU
```





# 9.1

准备完成：

1. 有没有一种好的获取history的方法

   已在stackOverFlow、github上提问，等待回答

   用Query能解决？

   麻了，用Query可能爆炸了

   我草！

2. cadence-system-worker信号机制（x）

3. （接1）可能需要自己构建history，所以要首先搞清楚cadence-system-worker所有的执行机制，才能在system-worker调度的过程中更新history。



# 9.2 

准备完成：

1. goroutine and channel 理解
2. callback funtion：1. callback function 的触发时间和机制 2. workflow在运行过程中会一直阻塞在callback function中吗 

3. History 接口获取慢的情况加了2G内存居然好了，艹

4. 将回调钩子搬移到了service层，更改了相应的单元测试

5. 了解了mock的新模式，我们开启mock后总会存在一些请求不想mock掉，而是真正的调用。设置`gock.EnableNetworking()`使得mock不匹配时可以绕过mock直接进行http请求，但是开启这个模式后会有bug，mock的地址对应的端口也必须有连接，换句话说我们不能mock自己随便编的一个网站，比如这样：

   ```go
   ➜  go_test go test mock2_test.go
   package main
   
   import (
           "testing"
           "github.com/ahuigo/requests"
           "gopkg.in/h2non/gock.v1"
   )
   
   func TestFoo(t *testing.T) {
           defer gock.Off()
           defer gock.DisableNetworking()
   
           // mock response
           gock.EnableNetworking()
           gock.New("http://123456").
                   Get("/bar").
                   Reply(200).
                   JSON(map[string]string{"foo": "bar"})
   
   
           // send request
           resp, err := requests.Get("http://123456/bar")
           if err != nil {
                   t.Fatal(err)
           }
           t.Log(resp.R.StatusCode)
           t.Log(resp.Text())
   }
   输出有问题：
   --- FAIL: TestFoo (0.00s)
       mock2_test.go:25: GET http://123456/bar Get "http://123456/bar": dial tcp 0.1.226.64:80: connect: no route to host
   FAIL
   FAIL	command-line-arguments	0.011s
   FAIL
   
   改成能真正访问的就好了：
   ➜  go_test go test mock2_test.go
   package main
   
   import (
           "testing"
   
           "github.com/ahuigo/requests"
           "gopkg.in/h2non/gock.v1"
   )
   
   func TestFoo(t *testing.T) {
           defer gock.Off()
           defer gock.DisableNetworking()
   
           // mock response
           gock.EnableNetworking()
           gock.New("http://pm-ui.hdmap.momenta.works").
                   Get("/bar").
                   Reply(200).
                   JSON(map[string]string{"foo": "bar"})
   
   
           // send request
           resp, err := requests.Get("http://pm-ui.hdmap.momenta.works/bar")
           if err != nil {
                   t.Fatal(err)
           }
           t.Log(resp.R.StatusCode)
           t.Log(resp.Text())
   }
   ```

   感觉是一个bug，按理说我mock的地址不应该一定要求要是能访问到的才对。



# 9.3

1. context与channel
2. 看cadence代码



# 9.4

1.  redis 广播 go-cache



# 9.6 

1. statement 的518和534行有对activityID的约束，要把_改为@. 研究清除replay机制，这样就能知道改之后再运行systemworker，之前的activityID会不会变化
2. Etc Redis hosts
3. Query and replay



# 9.7

1. Query 
   query开始的时候相当于直接对当前history进行重放吗？
   构造 1-->2-->3

   1. 运行完1-->2，正在跑3，此时query，查看query的整个流程（是不是重新模拟一个workflowTask放进去，然后被poller拉到，然后最后completed的时候进入handler返回情况？解决events的时候发生了什么）

   history replay的时候不会做什么，不会重新Execute？

2. eventID,taskID,workflowID,runID,state.除了eventID拿不到，其余均可以拿到

3. simple节点值得保留的信息：

```go
simpleNode：
name
workflowID
taksID
input
output
reason-------detail
startTime
endTime------
status-------可更新
owner task-def查出来的


choiceNode:
status
name
workflowID
children
type


substatus: 当前workflow信息

```

4. Start 的时序问题
   具体表现为，task执行的很快，没等信号发送到cadence-system-server就已经completed，这时协程会再次把状态置为started，造成错误。
   解决办法：start信号到达systemworker时，如果状态已经是completed，就不要在更新，换句话说，只有scheduled状态才能更新为"started"
5. query+history=100%replay
6. 到底要不要在执行的时候把dull节点的决策节点放到输出信息里？
7. Attempt 需要得到吗 拼成taskID的时候一定需要吗

8. 信号有可能宕机以后重新收到，那么有必要检查这个重新收到的信号所对应的task到底存不存在，以免出现nil-reference

```go
		WorkflowID string `json:"workflow_id"`
		RunID      string `json:"run_id"`
		TaskName   string `json:"task_name"`
		ActivityID string `json:"activity_id"`
		Attempt    string `json:"attempt"`

region_produce2:d2c4f96f-eba2-4411-8698-9b4df19bc2e1
6444b839-19b7-4ac3-8a28-2af2bf3e6d00
deploy_osm_db
deploy_osm_db
0
```

9. 关注流程图版本问题
10. binding里面究竟存了什么？

```yaml
keys:

-- QueryTaskInfos
-- workflow 
		-- workflow-Input
		-- workflo-ID
		-- run-ID
		-- searchAttr
		-- workflowDef
		-- loop_Count  记录每个task的循环次数
-- lastIpdateSearchAttributesTime
-- global
-- hxTask1Ref task执行的output，每一次task执行的output都放到binding中，方便其他task或判断条件调用
```



# 9.10

1. 构造标准测试用例---运行测试---得到测试结果choice loop join

2. 解决了两个bug，一个是loop下标问题，一个是缓存问题
3. 更新单元测试，create 支持 返回version



# 9.12

1. 除了dynamic fork 和 subworkflow，其余均已更新完毕
2. system-worker没有启动一般任务的activity-poller，启动的全是http的activity-poller



# 9.13

1. 解决reset
2. 解决http
3. 解决流程图的循环展示问题



# 9.14

1. retry机制与重复运行  retry涉及到replay吗 retry为什么不抛异常。（已解决）
2. 增加返回多种类型err-detail功能

3. http和wait测通

# 9.15

1. 任务定义流程定义DELETE文档编写(√)
2. skip功能的query跟进: 1. Skip 已经scheduled的，会出现两个scheduled      2.skip 还没有scheduled的 也会出现2个scheduled。
3. skip为什么会多次收到信号？经常多次收到信号，信号的消费机制究竟是怎样的？
4. Async 没有必要吧

# 9.16

进行压测

# 9.17

query-replay的原理

golang学习context





# 🐭人周报

# 9.26

今日计划：

1. 查看具体内存泄漏位置：确实是回调函数中的内存没有释放（√）
2. 进行system-worker内存泄漏修复：存在goroutine泄漏问题（√）
3. 进行心跳接口查表写（√）

有余力：

1. 进行history-replay的研究，看它泄露的goroutine到底停在了哪里！
2. 进行GC回收算法的学习



# 9.27

今日计划：

1. 查看stickyTime的具体定义和应用范围(√)
2. 研究并加Query缓存
3. 加单元测试(√)

有余力：

1. 进行GC回收算法的学习



# 9.28

今日计划：

1. mojito端加Query缓存(√)

2. 研究decisionTask的sticky继承范围

3. 流程失败子流程(√)



# 9.29

今日计划：

1. 支持流程图选择history和query中更快的那个(√)
2. 支持query查询限制大小



# 9.30

今日计划：

1. 支持query查询限制大小
2. 



# 10.10

发现并解决workflow中存在长时间task情况下的内存泄漏问题，通过调整中间件的stickyScheduledToStartTimeout参数和加缓存的方式，解决goroutine泄漏，进一步提高系统的稳定性和可用性





# 10.11

1. 完善提测单，将task 钩子重新放入提测项
2. redis查询策略：output,err=Gredis.get(key) 1.err!=nil 1.1 （redis.nil）不存在key（失效/没插过）-->查数据库写redis，返回数据库中value  1.2 报错 直接返回err 2.err==nil 直接返回value
3. lock



# 10.12

1.  



# 10.18

看sub-workflow的运行机制



操作系统

计算机网络

数据库

缓存

消息队列

实习内容

设计模式

分布式

java

golang





























